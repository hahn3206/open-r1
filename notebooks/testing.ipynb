{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import ast\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer, ModelConfig, ScriptArguments, TrlParser, get_peft_config\n",
    "\n",
    "\n",
    "def accuracy_reward(completions, solution, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    for content, sol in zip(contents, solution):\n",
    "        try:\n",
    "            # Regular expression to extract content between <answer> and </answer>\n",
    "            pattern = r\"<answer>(.*?)</answer>\"\n",
    "\n",
    "            # Find all matches\n",
    "            matches = re.findall(pattern, content, re.DOTALL)[0]\n",
    "            print(matches)\n",
    "            reward = 1.0 if sol in matches else 0.0\n",
    "        except Exception:  # if it fails for any reason, return 0.0\n",
    "            reward = 0.0\n",
    "        rewards.append(reward)\n",
    "    # Reward 1 if the content is the same as the ground truth, 0 otherwise\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, content, re.DOTALL) for content in completion_contents]\n",
    "    return [1.0 if match else 0.0 for match in matches]\n",
    "\n",
    "\n",
    "reward_funcs_registry = {\n",
    "    \"accuracy\": accuracy_reward,\n",
    "    \"format\": format_reward,\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 519.05 examples/s] \n",
      "Map: 100%|██████████| 400/400 [00:00<00:00, 580.16 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 88.66 examples/s]\n",
      "Map: 100%|██████████| 400/400 [00:00<00:00, 1492.57 examples/s]\n",
      "Map: 100%|██████████| 400/400 [00:00<00:00, 1004.49 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 86.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    " # Load the dataset\n",
    "dataset = load_dataset('lordspline/arc-agi')\n",
    "\n",
    "# Format into conversation\n",
    "def make_conversation(example):\n",
    "    examples = ''\n",
    "    ex_num = 1\n",
    "    for ex in example['train']:\n",
    "        ex_in = \"\\n\".join(\" \".join(map(str, row)) for row in ex['input'])\n",
    "        ex_out = \"\\n\".join(\" \".join(map(str, row)) for row in ex['output'])\n",
    "        examples += f'Example {ex_num}: \\n\\nInput:\\n{ex_in}\\nOutput:\\n{ex_out}\\n\\n'\n",
    "        ex_num += 1\n",
    "    test_in = \"\\n\".join(\" \".join(map(str, row)) for row in example[\"test\"][0][\"input\"])\n",
    "\n",
    "    question = f'Find the common rule that maps an input grid to an output grid, given the examples below.\\n{examples} Below is a test input grid. Predict the corresponding output grid by applying the rule you found. Your final answer should just be the text output grid itself. \\n\\nInput:\\n{test_in}\\n'\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def make_solution(example):\n",
    "    solution = \"\\n\".join(\" \".join(map(str, row)) for row in example[\"test\"][0][\"output\"])\n",
    "    return {\n",
    "        \"solution\": f'{solution}'\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(make_conversation)\n",
    "dataset = dataset.map(make_solution)\n",
    "dataset = dataset.remove_columns(\"train\")\n",
    "dataset = dataset.remove_columns(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the common rule that maps an input grid to an output grid, given the examples below.\n",
      "Example 1: \n",
      "\n",
      "Input:\n",
      "0 0 5\n",
      "0 5 0\n",
      "5 0 0\n",
      "Output:\n",
      "3 3 3\n",
      "4 4 4\n",
      "2 2 2\n",
      "\n",
      "Example 2: \n",
      "\n",
      "Input:\n",
      "0 0 5\n",
      "0 0 5\n",
      "0 0 5\n",
      "Output:\n",
      "3 3 3\n",
      "3 3 3\n",
      "3 3 3\n",
      "\n",
      "Example 3: \n",
      "\n",
      "Input:\n",
      "5 0 0\n",
      "0 5 0\n",
      "5 0 0\n",
      "Output:\n",
      "2 2 2\n",
      "4 4 4\n",
      "2 2 2\n",
      "\n",
      "Example 4: \n",
      "\n",
      "Input:\n",
      "0 5 0\n",
      "0 0 5\n",
      "0 5 0\n",
      "Output:\n",
      "4 4 4\n",
      "3 3 3\n",
      "4 4 4\n",
      "\n",
      " Below is a test input grid. Predict the corresponding output grid by applying the rule you found. Your final answer should just be the text output grid itself. \n",
      "\n",
      "Input:\n",
      "0 0 5\n",
      "5 0 0\n",
      "0 5 0\n",
      "\n",
      "3 3 3\n",
      "2 2 2\n",
      "4 4 4\n"
     ]
    }
   ],
   "source": [
    "print(dataset['training'][0]['prompt'][1]['content'])\n",
    "print(dataset['training'][0]['solution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "3 3 3\n",
      "2 2 2\n",
      "4 4 2 \n",
      "[0.0]\n",
      "[1.0]\n"
     ]
    }
   ],
   "source": [
    "answer = [[{'role': 'assistant', 'content': '''<think> 1, 2, 3, 4, 5 </think><answer> \n",
    "3 3 3\n",
    "2 2 2\n",
    "4 4 2 </answer>'''\n",
    "}]]\n",
    "\n",
    "print(accuracy_reward(answer, [dataset['training'][0]['solution']]))\n",
    "\n",
    "print(format_reward(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
